norms = norms)
convergence <- 1
}
while(convergence == 0 & iter < maxiter){
if(abs(det(DFun(current_beta))) <= eps){
norms <- c(norms,sqrt(sum(Fun(current_beta) * Fun(current_beta))))
res <- list(beta = current_beta,
convergence = 0,
iter = iter + 1,
norms = norms)
convergence <- 1
}else{
#current_beta <- current_beta - solve(DFun(current_beta)) %*% Fun(current_beta)
current_beta <- current_beta - solve(DFun(beta)) %*% Fun(beta)
norms <- c(norms,sqrt(sum(Fun(current_beta) * Fun(current_beta))))
iter <- iter + 1
if(sum(Fun(current_beta) * Fun(current_beta)) <= eps){
res <- list(beta = current_beta,
convergence = 1,
iter = iter,
norms = norms)
convergence <- 1
}
}
}
if(iter >= maxiter){
norms[[maxiter]] <- sum(Fun(current_beta) * Fun(current_beta))
res <- list(beta = current_beta,
convergence = 0,
iter = maxiter,
norms = c(norms,sqrt(sum(Fun(current_beta) * Fun(current_beta)))))
}
return(res)
}
beta_hat <- Newton(beta, maxiter = 100)
beta_hat
norms
norms <- 4
norms %>% length()
norms[length(norms)]
# We write a function to do Newton's method
# following the previously described pseudo-code
Newton <- function(beta0, maxiter = 1000, eps = 1e-50){
# Initialise the results
current_beta <- beta0
convergence <- 0
iter <- 0
norms <- sqrt(sum(Fun(current_beta) * Fun(current_beta)))
# Start loop to convergence
while(convergence == 0 & iter < maxiter & norms[length(norms)] > eps){
# Check if the derivative of F is singular
if(abs(det(DFun(current_beta))) <= eps){
# Return current beta with no convergence
res <- list(beta = current_beta,
convergence = 0,
iter = iter + 1,
norms = norms)
# Claim it has converged to get out of loop
convergence <- 1
}else{
# We have a non-singular matrix, so we proceed
# Update beta
current_beta <- current_beta - solve(DFun(current_beta)) %*% Fun(current_beta)
# Update norm vector, this is for the history of convergence
# Recall we should find a quadratic convergence
norms <- c(norms,sqrt(sum(Fun(current_beta) * Fun(current_beta))))
# Update number of iterations
iter <- iter + 1
# Check if we have converged
if(sqrt(sum(Fun(current_beta) * Fun(current_beta))) <= eps){
# Update convergence
convergence <- 1
# This is what we want to find
res <- list(beta = current_beta,
convergence = 1,
iter = iter,
norms = norms)
}
}
}
# Check if we ran out of iterations
if(iter >= maxiter){
# Return what we have with no convergence
res <- list(beta = current_beta,
convergence = 0,
iter = iter,
norms = norms)
}
# Only one return!
return(res)
}
beta_hat <- Newton(beta, maxiter = 100)
beta_hat
beta_hat <- Newton(beta, maxiter = 100, eps = 1e-50)
beta_hat
beta
plot(beta_hat$norms,type='l')
# We then define the function F
Fun <- function(beta,G = 1e13){
res <- (X %>% as.matrix() %>% t()) %*% (y - p(beta))
res <- as.numeric(G * res)
return(res)
}
Fun(beta)
Fun(log_reg$coefficients)
# We then define the function F
Fun <- function(beta,G = 1e6){
res <- (X %>% as.matrix() %>% t()) %*% (y - p(beta))
res <- as.numeric(G * res)
return(res)
}
Fun(beta)
Fun(log_reg$coefficients)
plot(p(log_reg$coefficients),df$fitted)
plot(p(c(0,0,0)),df$fitted)
# Define how p depends on X and beta.
# Note that we will consider X to be fixed and we will be interested in p
# just as a function of beta.
p <- function(beta){
return(1/(1 + exp(-colSums(apply(X,1,function(k){return(k*beta)})))))
}
# We write a function to do Newton's method
# following the previously described pseudo-code
Newton <- function(beta0, maxiter = 1000, eps = 1e-50){
# Initialise the results
current_beta <- beta0
convergence <- 0
iter <- 0
norms <- sqrt(sum(Fun(current_beta) * Fun(current_beta)))
# Start loop to convergence
while(convergence == 0 & iter < maxiter & norms[length(norms)] > eps){
# Check if the derivative of F is singular
if(abs(det(DFun(current_beta))) <= eps){
# Return current beta with no convergence
res <- list(beta = current_beta,
convergence = 0,
iter = iter + 1,
norms = norms)
# Claim it has converged to get out of loop
convergence <- 1
}else{
# We have a non-singular matrix, so we proceed
# Update beta
current_beta <- current_beta - solve(DFun(current_beta)) %*% Fun(current_beta)
# Update norm vector, this is for the history of convergence
# Recall we should find a quadratic convergence
norms <- c(norms,sqrt(sum(Fun(current_beta) * Fun(current_beta))))
# Update number of iterations
iter <- iter + 1
# Check if we have converged
if(sqrt(sum(Fun(current_beta) * Fun(current_beta))) <= eps){
# Update convergence
convergence <- 1
# This is what we want to find
res <- list(beta = current_beta,
convergence = 1,
iter = iter,
norms = norms)
}
}
}
# Check if we ran out of iterations
if(iter >= maxiter){
# Return what we have with no convergence
res <- list(beta = current_beta,
convergence = 0,
iter = iter,
norms = norms)
}
# Only one return!
return(res)
}
beta_hat <- Newton(beta, maxiter = 100, eps = 1e-50)
beta_hat
plot(p(log_reg$coefficients),df$fitted)
plot(p(c(0,0,0)),df$fitted)
plot(df$fitted,p(log_reg$coefficients))
plot(df$fitted,p(c(0,0,0)))
beta
Fun(beta)
Fun(log_reg$coefficients)
DFun(beta)
DFun(log_reg$coefficients)
det(DFun(beta))
det(DFun(log_reg$coefficients))
beta_hat <- Newton(beta, maxiter = 100, eps = 1e-5)
beta_hat
Fun(beta_hat$beta)
det(DFun(beta_hat$beta))
beta0
beta0 <- c(0,0,0)
Fun(beta0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
# We then define the function F
Fun <- function(beta){
res <- (X %>% as.matrix() %>% t()) %*% (y - p(beta))
res <- as.numeric(res)
return(res)
}
beta0 <- c(0,0,0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
beta0 <- beta0 - solve(DFun(beta0)) %*% Fun(beta0)
beta0
Fun(beta0)
beta
beta_hat <- Newton(beta, maxiter = 100, eps = 1e-10)
beta_hat
Fun(beta_hat$beta)
det(DFun(beta_hat$beta))
plot(beta_hat$norms)
beta_hat <- Newton(beta)
Fun(beta_hat$beta)
beta_hat
beta_hat$beta
Fun(beta_hat$beta)
beta_hat <- Newton(beta, maxiter = 1000, eps = 1e-10)
beta_hat$beta
Fun(beta_hat$beta)
plot(beta_hat$norms)
cbind(beta_hat$beta,log_reg$coefficients)
data.frame( our_coefficients = beta_hat$beta,
R_function = log_reg$coefficients)
boxplot(fitted~y,data=df)
grid <- expand.grid(list(x1 = seq(0,1,length = 100), x2 = seq(0,1,length = 100)))
surface <-  1/(1+exp(-predict(log_reg,newdata = grid)))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
grid <- expand.grid(list(x1 = seq(0,1,length = 100), x2 = seq(0,1,length = 100)))
surface <-  1/(1+exp(-predict(log_reg,newdata = grid)))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = df$y + 1, pch = 15)
grid <- expand.grid(list(x1 = seq(0,1,length = 100), x2 = seq(0,1,length = 100)))
surface <-  1/(1+exp(-predict(log_reg,newdata = grid)))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = df$y + 1, pch = 15)
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c(1,2),pch = 15, xpd = TRUE, bty = 'n')
?image
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = heat.colors(1),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = heat.colors(2),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = heat.colors(15),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = heat.colors(12),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
?rainbow
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = rainbow(3),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = rainbow(12),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = rainbow(7),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = rainbow(20),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
?rainbow
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = topo.colors(15),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c(1,2),pch = 15, xpd = TRUE, bty = 'n')
points(df$x1,df$x2,col = df$y + 1, pch = 15)
topo.colors(15)
heat.colors(15)
heat.colors(15) %>% rev()
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100), col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = df$y + 1, pch = 15)
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c(1,2),pch = 15, xpd = TRUE, bty = 'n')
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = c('black','blue'), pch = 15)
df$y
lapply(df$y,function(x){ return(if_else(x==1,'black','green'))})
unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','green'))}))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','green'))})),
pch = 15)
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c('black','green'),pch = 15, xpd = TRUE, bty = 'n')
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','green'))})),
pch = 15)
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c('green','black'),pch = 15, xpd = TRUE, bty = 'n')
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','blue'))})),
pch = 15)
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c('green','blue'),pch = 15, xpd = TRUE, bty = 'n')
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','blue'))})),
pch = 15)
legend(1.05, 0.6, c('y = 1', 'y = 0'),col = c('blue','black'),pch = 15, xpd = TRUE, bty = 'n')
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','blue'))})),
pch = 15)
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c('blue','black'),pch = 15, xpd = TRUE, bty = 'n')
points(df$x1,df$x2,col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','blue'))})),
pch = 15,
xlab = 'x1',ylab = 'x2', main = 'Fake data', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,
col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','blue'))})),
pch = 15,
xlab = 'x1',ylab = 'x2', main = 'Fake data', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2)
points(df$x1,df$x2)
# Data
df <- read.csv(paste0(getwd(),'/data/data.csv'))
plot(df$x1,df$x2,col=df$y + 1,pch = 15,
xlab = 'x1',ylab = 'x2', main = 'Fake data', asp = 1, xlim = c(0,1), ylim = c(0,1))
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c(1,2),pch = 15, xpd = TRUE, bty = 'n')
points(df$x1,df$x2)
points(df$x1,df$x2)
plot(df$x1,df$x2,
col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','blue'))})),
pch = 15,
xlab = 'x1',ylab = 'x2', main = 'Fake data', asp = 1, xlim = c(0,1), ylim = c(0,1))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
grid <- expand.grid(list(x1 = seq(0,1,length = 100), x2 = seq(0,1,length = 100)))
surface <-  1/(1+exp(-predict(log_reg,newdata = grid)))
# Logistic regression
log_reg <- glm(y ~ x1 + x2,family = 'binomial', data = df)
df$probabilities <- log_reg$fitted.values
boxplot(probabilities~y,data=df)
grid <- expand.grid(list(x1 = seq(0,1,length = 100), x2 = seq(0,1,length = 100)))
surface <-  1/(1+exp(-predict(log_reg,newdata = grid)))
image(seq(0,1,length = 100),seq(0,1,length = 100),matrix(surface,ncol = 100),
col = rev(heat.colors(15)),
xlab = 'x1',ylab = 'x2', main = 'Logistic regression', asp = 1, xlim = c(0,1), ylim = c(0,1))
points(df$x1,df$x2,col = unlist(lapply(df$y,function(x){ return(if_else(x==1,'black','blue'))})),
pch = 15)
legend(1.05, 0.6, c('y = 0', 'y = 1'),col = c('blue','black'),pch = 15, xpd = TRUE, bty = 'n')
contour(x = seq(0,1,length = 100),
y = seq(0,1,length = 100),
z = matrix(surface,ncol = 100),
levels = c(0.1,0.5,0.9),
add = TRUE)
df[df$probabilities <= 0.9 & df$probabilities >= 0.1,]
df[df$probabilities <= 0.9 & df$probabilities >= 0.1,] %>% dim()
plot(df$x1[df$probabilities <= 0.9 & df$probabilities >= 0.1],
df$x2[df$probabilities <= 0.9 & df$probabilities >= 0.1],
col = 1+df$y[df$probabilities <= 0.9 & df$probabilities >= 0.1])
plot(df$x1[df$probabilities <= 0.9 & df$probabilities >= 0.1], pch =15,
df$x2[df$probabilities <= 0.9 & df$probabilities >= 0.1],
col = 1+df$y[df$probabilities <= 0.9 & df$probabilities >= 0.1])
log_reg$boundary
log_reg$xlevels
log_reg$terms
log_reg$R
log_reg$effects
log_reg$residuals
plot(log_reg$residuals)
df$fitted <- df$probabilities %>%
lapply(function(p){ return(if_else(p>=0.5,1,0)) }) %>%
unlist()
head(df)
?xtabs
xtabs(y~fitted,data = df)
sum(df$fitted[df$y==1]==1)
sum(df$fitted[df$y==0]==0)
47+46
sum(df$fitted[df$y==1]==0)
sum(df$fitted[df$y==0]==1)
xtabs(y~fitted,data = df)
xtabs(y~1-fitted,data = df)
xtabs(y~(1-fitted),data = df)
xtabs(1-y~fitted,data = df)
xtabs(~y +fitted,data = df)
47+43
47/(47+7)
sd(df$y-df$fitted)
mean(sum((df$y-df$fitted)^2)
)
mean(sum((df$y-df$fitted)^2))
(df$y-df$fitted)^2
sum((df$y-df$fitted)^2)
mean(sum((df$y-df$fitted)^2))
mean((df$y-df$fitted)^2)
sd(df$y-df$fitted)
sqrt(mean((df$y-df$fitted)^2))
99/100*sd(df$y-df$fitted)
sqrt(mean((df$y-df$fitted)^2))
sqrt(mean((df$y-df$fitted)^2))
sd(df$y-df$fitted)/
sqrt(mean((df$y-df$fitted)^2))
sqrt(mean((df$y-df$fitted)^2))/
sd(df$y-df$fitted)
(df$y-df$fitted)^2
mean((df$y-df$fitted)^2)
mean((df$y-df$fitted)^2)
var(df$y-df$fitted)
99*var(df$y-df$fitted)
99/100*var(df$y-df$fitted)
sqrt(mean((df$y-df$fitted)^2))
sqrt(99/100*var(df$y-df$fitted))
var(1)
var(c(1,2))
mean((c(1,2)-1.5)^2)
2*mean((c(1,2)-1.5)^2)
sqrt(2*mean((c(1,2)-1.5)^2))
sd(c(1,2))
xtabs(~y +fitted,data = df)
xtabs(~y +fitted,data = df)[1,1]
xtabs(~y +fitted,data = df)[1,2]
xtabs(~y +fitted,data = df)[2,2]
seq(0.1,0.9,0.01)
lapply(Calif$cut_off,function(p){
aux <- df$probabilities %>% lapply(function(p){ return(if_else(p>=0.5,1,0)) }) %>% unlist()
cont_tab <- xtabs(~df$y + aux)
Correct <- cont_tab[1,1] + cont_tab[2,2]
Jac <- cont_tab[2,2]/(cont_tab[2,2] + cont_tab[1,2] + cont_tab[2,1])
res <- data.frame(Correct = Correct,
Jaccard = Jaccard)
return(res)
}) %>% do.call(rbind,.)
Calif <- data.frame(cut_off = seq(0.1,0.9,0.01))
lapply(Calif$cut_off,function(p){
aux <- df$probabilities %>% lapply(function(p){ return(if_else(p>=0.5,1,0)) }) %>% unlist()
cont_tab <- xtabs(~df$y + aux)
Correct <- cont_tab[1,1] + cont_tab[2,2]
Jac <- cont_tab[2,2]/(cont_tab[2,2] + cont_tab[1,2] + cont_tab[2,1])
res <- data.frame(Correct = Correct,
Jaccard = Jaccard)
return(res)
}) %>% do.call(rbind,.)
lapply(Calif$cut_off,function(p){
aux <- df$probabilities %>% lapply(function(p){ return(if_else(p>=0.5,1,0)) }) %>% unlist()
cont_tab <- xtabs(~df$y + aux)
Correct <- cont_tab[1,1] + cont_tab[2,2]
Jac <- cont_tab[2,2]/(cont_tab[2,2] + cont_tab[1,2] + cont_tab[2,1])
res <- data.frame(Correct = Correct,
Jaccard = Jac)
return(res)
}) %>% do.call(rbind,.)
Calif <- data.frame(cut_off = seq(0.01,0.99,0.001))
lapply(Calif$cut_off,function(p){
aux <- df$probabilities %>% lapply(function(p){ return(if_else(p>=0.5,1,0)) }) %>% unlist()
cont_tab <- xtabs(~df$y + aux)
Correct <- cont_tab[1,1] + cont_tab[2,2]
Jac <- cont_tab[2,2]/(cont_tab[2,2] + cont_tab[1,2] + cont_tab[2,1])
res <- data.frame(Correct = Correct,
Jaccard = Jac)
return(res)
}) %>% do.call(rbind,.)
Calif <- data.frame(cut_off = seq(0.01,0.99,0.001))
Calif <- lapply(Calif$cut_off,function(p){
aux <- df$probabilities %>% lapply(function(p){ return(if_else(p>=0.5,1,0)) }) %>% unlist()
cont_tab <- xtabs(~df$y + aux)
Correct <- cont_tab[1,1] + cont_tab[2,2]
Jac <- cont_tab[2,2]/(cont_tab[2,2] + cont_tab[1,2] + cont_tab[2,1])
res <- data.frame(Correct = Correct,
Jaccard = Jac)
return(res)
}) %>% do.call(rbind,.) %>% cbind(Calif,.)
head(Calif)
Calif <- data.frame(cut_off = seq(0.01,0.99,0.001))
Calif <- lapply(1:length(Calif$cut_off),function(k){
aux <- df$probabilities %>%
lapply(function(p){ return(if_else(p>=Calif$cut_off[k],1,0)) }) %>% unlist()
cont_tab <- xtabs(~df$y + aux)
Correct <- cont_tab[1,1] + cont_tab[2,2]
Jac <- cont_tab[2,2]/(cont_tab[2,2] + cont_tab[1,2] + cont_tab[2,1])
res <- data.frame(Correct = Correct,
Jaccard = Jac)
return(res)
}) %>% do.call(rbind,.) %>% cbind(Calif,.)
head(Calif)
plot(Calif$cut_off,Calif$Correct)
plot(Calif$cut_off,Calif$Jaccard)
plot(Calif$cut_off,Calif$Correct)
plot(Calif$cut_off,Calif$Correct/100,type='l')
lines(Calif$cut_off,Calif$Jaccard,col='red')
df$fitted <- df$probabilities %>%
lapply(function(p){ return(if_else(p>=0.6,1,0)) }) %>%
unlist()
xtabs(y~fitted,data = df)
xtabs(~y +fitted,data = df)
49 + 45
which(Calif$Correct == max(Calif$Correct))
Calif$cut_off[which(Calif$Correct == max(Calif$Correct))]
